<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhiqiang Yan</title>

  <meta name="author" content="Zhiqiang Yan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cmu-seal-r.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhiqiang Yan</name>
                  </p>
                  <p>I am a Phd student at <a href="http://www.patternrecognition.asia/">PCALab</a> of <a
                      href="https://www.njust.edu.cn/">Nanjing University of Science and Technology</a> (NJUST) from 2020 to 2024, advised by Prof.  
			  <a
                      href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a> 
			  and co-advised by Prof. 
                    <a href="https://sites.google.com/view/junlineu/">Jun Li. </a>
                  </p>
		  <p>I obtained my B.E. degree in 2018 from NJUST. 
		  </p>
                  <p style="text-align:center">
                    <a href="mailto:yanzq@njust.edu.cn"> Email </a> &nbsp/&nbsp
                    <a href="images/CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://github.com/yanzq95"> GitHub </a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=hnrkzIEAAAAJ&hl=zh-CN&oi=sra"> Google Scholar </a> &nbsp/&nbsp
		    <a href="images/wechat_yanzq.jpg"> WeChat </a> 
                  </p>
		  <p>
			  <font color='red'>I am looking for a postdoctoral position that will allow me to continue my research and expand my academic skills and network. I expect to graduate in June 2024. 
				  If you are interested in my profile, please contact me. Thank you.</font>
		  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a><img style="width:200px;max-width:100%" alt="profile photo" src="images/yanzq.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    My research interests include computer vision and machine learning, especially on depth estimation, depth completion, and depth super-resolution. These tasks are crucial for various applications, 
			  such as self-driving, robotic vision, and related 3D visual perception. I am also fascinated by the task of 3D occupancy prediction.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/SPFNet/SPFNet.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2402.13876.pdf">Scene Prior Filtering for Depth Map Super-Resolution</a>
                  </papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang*</a>, 
			  <strong>Zhiqiang Yan*</strong> &#9993, 
			  <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, 
			  <a href="https://jspan.github.io/">Jinshan Pan</a>, 
			  <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>, 
			  <a href="https://tyshiwo.github.io/">Ying Tai</a>, 
			  <a href="https://guangweigao.github.io/">Guangwei Gao</a>
                  <br> <br>
                  <em>arXiv</em>, 2024, <a href="projectpage/SPFNet/index.html">project page</a>
                  <br>
                  <p></p>
                  <p>To address the issues of texture interference and edge inaccuracy in GDSR, for the first time, SPFNet introduces the priors surface normal and semantic map from large-scale models.
				 As a result, SPFNet achieves state-of-the-art performance.</p>
                </td>
              </tr>

            </tbody>
          </table>

		
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/SGNet/SGNet_performance.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2312.05799v3.pdf">SGNet: Structure Guided Network via Gradient-Frequency Awareness for Depth Map Super-Resolution</a>
                  </papertitle>
                  <br>
                  <a href="https://scholar.google.com/citations?user=VogTuQkAAAAJ&hl=zh-CN&oi=sra">Zhengxue Wang</a>, 
			  <strong>Zhiqiang Yan</strong> &#9993, 
		  	  <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>AAAI</em>, 2024, <a href="projectpage/SGNet/index.html">project page</a>
                  <br>
                  <p></p>
                  <p>SGNet introduces a novel perspective that exploits the gradient and frequency domains for the structure enhancement of DSR task,
				  surpassing the five state-of-the-art methods by 16% (RGB-D-D), 24% (Middlebury), 21% (Lu) and 15% (NYU-v2) in average.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/DUL/DUL.png" width="300">&nbsp 
			<img src="images/DUL/DUL-vis.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://openreview.net/pdf?id=0tLjOxqjLS">Distortion and Uncertainty Aware Loss for Panoramic Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="https://shuochenya.github.io/">Shuo Chen &#9993</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>
                  <br> <br>
                  <em>ICML</em>, 2023
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>Standard MSE or MAE loss function is commonly used in limited field-of-vision depth completion, treating each pixel equally under a basic assumption that all pixels have same contribution during optimization. 
			  However, the assumption is inapplicable to panoramic data due to its latitude-wise distortion and high uncertainty nearby textures and edges. 
			  To handle these challenges, this paper proposes the distortion and uncertainty aware loss (DUL) that consists of a distortion-aware loss and an uncertainty-aware loss.</p>
                </td>
              </tr>

            </tbody>
          </table>
		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
		    <img src="images/DesNet/DesNet-vis.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://ojs.aaai.org/index.php/AAAI/article/view/25415/25187">DesNet: Decomposed Scale-Consistent Network for Unsupervised Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>AAAI</em>, 2023, <strong>oral</strong>
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>DesNet first introduces a decomposed scale-consistent learning strategy, which disintegrates the absolute depth into relative depth prediction and global scale estimation, contributing to individual learning benefits. 
		  Extensive experiments show the superiority of DesNet on KITTI benchmark, ranking 1st and surpassing the second best more than 12% in RMSE.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/RigNet/RigNet.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2107.13802.pdf">RigNet: Repetitive Image Guided Network for Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>ECCV</em>, 2022
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>RigNet explores a repetitive design for depth completion to tackle the blurry guidance in image and unclear structure in depth. 
				  Extensive experiments show that RigNet achieves superior or competitive results on KITTI benchmark and NYUv2 dataset.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/MMMPT/MMMPT.jpg" width="300">&nbsp 
			<img src="images/MMMPT/MMMPT-vis.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://arxiv.org/pdf/2203.09855.pdf">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan*</strong>, 
			<a href="http://implus.github.io/">Xiang Li*</a>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang &#9993</a>
                  <br> <br>
                  <em>ECCV</em>, 2022
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>For the first time, we enable the masked pre-training in a Convolution-based multi-modal task, instead of the Transformer-based single-modal task. 
		  What's more, we introduce the panoramic depth completion, a new task that facilitates 3D reconstruction.</p>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="nsdnet_stop()" onmouseover="nsdnet_start()"> <!-- bgcolor="#ffffd0" -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/IDSR/IDSR.jpg" width="300">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle><a href="https://drive.google.com/file/d/1qMOQz2sfci_ifRO2aSW5KyyDFKwWvLSW/view">Learning Complementary Correlations for Depth Super-Resolution with Incomplete Data in Real World</a>
                  </papertitle>
                  <br>
                  <strong>Zhiqiang Yan</strong>, 
			<a href="https://scholar.google.com/citations?user=ORn7aZcAAAAJ&hl=zh-CN&oi=sra">Kun Wang</a>, 
			<a href="http://implus.github.io/">Xiang Li</a>, 
			<a href="https://jessezhang92.github.io/">Zhenyu Zhang</a>, 
			<a href="https://scholar.google.com/citations?user=qovg0wcAAAAJ&hl=zh-CN&oi=ao">Guangyu Li &#9993</a>, 
			<a href="https://sites.google.com/view/junlineu/">Jun Li &#9993</a>, 
		  	<a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Jian Yang</a>
                  <br> <br>
                  <em>TNNLS</em>, 2022
                  <br>
		<!-- <p>
                  <a href="projectpage/NSDNet/index.html">project page</a>
		</p> -->
                  <p></p>
                  <p>Motivated by pratical applications, this paper introduces a new task, i.e., incomplete depth super-resolution (IDSR), which recovers dense and high-resolution depth from incomplete and low-resolution one.</p>
                </td>
              </tr>

            </tbody>
          </table>
		

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Honors and Awards</heading>
                  <ul>
		    <li>2023.10, National Scholarship (Top 2%), NJUST;</li>
                  </ul>
                  <ul>
                    <li>2022.10, Hua Wei Scholarship (Top 1%), NJUST;</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    This webpage is forked from <a href="https://github.com/fanjunkai1/fanjunkai1.github.io">Junkai Fan</a>. Thanks to him!
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
